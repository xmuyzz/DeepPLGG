#----------------------------------------------------------------------
# Deep learning for classification for contrast CT;
# Transfer learning using Google Inception V3;
#-------------------------------------------------------------------------------------------

import os
import numpy as np
import pandas as pd
import seaborn as s1n
import matplotlib.pyplot as plt
import glob
import tensorflow
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.image import img_to_array, load_img, ImageDataGenerator
from tensorflow.keras.layers import GlobalAveragePooling2D
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.applications import EfficientNetB5
from tensorflow.keras.applications import EfficientNetB4
from tensorflow.keras.applications import EfficientNetB3
from tensorflow.keras.applications import DenseNet121
# ----------------------------------------------------------------------------------
# transfer learning CNN model
# ----------------------------------------------------------------------------------
def EffNet_model(input_shape, batch_momentum, activation,
                    activation_out, loss_function, optimizer, dropout_rate):

    ### Keras CNN models for use: https://keras.io/api/applications/
    ### InceptionV3(top1 acc 0.779)
    ### InceptionResnetV2(top1 acc 0.803),
    ### ResNet152V2(top1 acc 0.780)
    ### EficientNetB4

    base_model = DenseNet121(
        weights=None,    # 'imagenet'
        include_top=False,
        input_shape=input_shape
        )
    print('base model')
                                 
    out = base_model.output
    out = BatchNormalization(momentum=batch_momentum)(out)
    out = GlobalAveragePooling2D()(out)
    
    out = BatchNormalization(momentum=batch_momentum)(out)
    out = Dense(512, activation=activation)(out)
    out = Dropout(dropout_rate)(out)
    
    out = BatchNormalization(momentum=batch_momentum)(out)
    out = Dense(128, activation=activation)(out)
    out = Dropout(dropout_rate)(out)

    out = BatchNormalization(momentum=batch_momentum)(out)
    predictions = Dense(1, activation=activation_out)(out)

    model = Model(inputs=base_model.input, outputs=predictions)

    # only if we want to freeze layers
    for layer in base_model.layers:
        layer.trainable = False	

    print('complie model')
        
    model.compile(
                  loss=loss_function,
                  optimizer=optimizer,
                  metrics=['accuracy']
                  )
    
    model.summary()

    return model





    

    
